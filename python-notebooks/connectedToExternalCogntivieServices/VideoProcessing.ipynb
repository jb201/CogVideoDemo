{"cells":[{"cell_type":"code","source":["import cv2\nimport requests \nimport uuid\nimport os\nimport time\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n# shouls be stored in databricks secrets backed by Azure Databrick secrets\n# \n# replace with your subscription key for face api \n# subscription_key = '3a991cb25c2347db81db939c957ce715'\n# premium key \nsubscription_key = '90773c9afa3d4b2195059d2a0f16e722'\nassert subscription_key\n\n\n# replace below with the correct location of face api account (need to test with containerized version)\nface_api_url = 'https://eastus2.api.cognitive.microsoft.com/face/v1.0/detect?returnFaceLandmarks=true'\nheaders = { 'Ocp-Apim-Subscription-Key': subscription_key }\n\n# adjust to your parameter set     \nparams = {\n    'returnFaceId': 'true',\n    'returnFaceLandmarks': 'false',\n    'returnFaceAttributes': 'age,gender,headPose,smile,emotion,hair,makeup,occlusion,accessories,blur,exposure,noise',\n}"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%run \"./PyTorch Emotion Eval\""],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["def process_frame_by_face_api(frame):\n    headers = { 'Content-Type':'application/octet-stream','Ocp-Apim-Subscription-Key': subscription_key }\n    ret, buf = cv2.imencode('.jpg', frame)\n    response = requests.post(face_api_url, params=params, headers=headers, data=buf.tobytes())\n    res = response.json()\n    return res"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql.functions import from_json\nfrom pyspark.sql.types import *\nfrom pyspark.sql import *\nimport pandas as pd \nfrom pandas.io.json import json_normalize\n\ndef persistFaceToTable(faces, videoId, tableName):\n  dfa = None\n  print( len(faces))\n  count = 0\n  for faceResult in faces:\n     flat_record = flatten_json(faceResult[2])\n     b = json_normalize(flat_record)\n     if len(flat_record) != 98:\n       print(len(flat_record))\n       print('odd sized reocrd, skipping')\n       continue\n     b = b.assign(frameId=str(faceResult[0]))\n     b = b.assign(frameNum=faceResult[1])\n     b = b.assign(videoId=videoId)\n     torchVals = faceResult[3].data.tolist()\n     b = b.assign(torchAnger=torchVals[0])\n     b = b.assign(torchDisgust=torchVals[1])\n     b = b.assign(torchFear=torchVals[2])\n     b = b.assign(torchHappy=torchVals[3])\n     b = b.assign(torchSad=torchVals[4])\n     b = b.assign(torchSurprise=torchVals[5])\n     b = b.assign(torchNeutral=torchVals[6])\n     df = spark.createDataFrame(b)\n     count = count + 1\n     if dfa == None:\n       dfa = df\n     else:\n       dfa = dfa.union(df)\n  dfa.write.mode(\"append\").saveAsTable(tableName)\n  dfa = None\n  "],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["def process_sample_frames(video_filename, sample, videoId):\n    \"\"\"Extract frames from video\"\"\"\n    cap = cv2.VideoCapture(video_filename)\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n    faces = []\n    persist_count = 20\n    \n    print('video length/frame count: ', video_length)\n    if cap.isOpened() and video_length > 0:\n        success, image = cap.read()\n        count = 0\n        collectCount = 0\n        while success:\n            if count % sample == 0:\n                \"\"\" process \"\"\"\n                face_result = process_frame_by_face_api(image) \n                # print(len(face_result))\n                faces_found = len(face_result)\n                for i in (range(0,faces_found)):\n                    newId = uuid.uuid4()\n                    x = face_result[i]['faceRectangle']['left']\n                    y= face_result[i]['faceRectangle']['top']\n                    h = face_result[i]['faceRectangle']['height']\n                    w = face_result[i]['faceRectangle']['width']\n                    crop_img = crop_img = image[y:y+h, x:x+w]\n                    scores = eval_pyTorch_emotion(crop_img)\n                    id = face_result[i]['faceId']\n                    val = (newId, count, face_result[i], scores)\n                    faces.append(val)                    \n                collectCount += 1\n                # so not to go over 10 frame requests per sec\n                if collectCount % 8 == 0:\n                  time.sleep(1.0)\n            if len(faces) >= persist_count and len(faces) != 0:\n              persistFaceToTable(faces, videoId, \"tempFaceTable\")\n              faces = []\n            success, image = cap.read()\n            count += 1\n        if len(faces) > 0:\n          persistFaceToTable(faces, videoId, \"tempFaceTable\")\n          faces = []\n    return faces"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#results = process_sample_frames('/dbfs/mnt/bs/IncomingVideos/making_face.mp4', 20)\n#print (results)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#print(len(results))\n\n\n                   "],"metadata":{},"outputs":[],"execution_count":7}],"metadata":{"name":"VideoProcessing","notebookId":270093129287848},"nbformat":4,"nbformat_minor":0}

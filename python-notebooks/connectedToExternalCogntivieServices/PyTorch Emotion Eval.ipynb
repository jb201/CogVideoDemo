{"cells":[{"cell_type":"code","source":["# loads the model built from here and evaluates faces\n# https://github.com/WuJie1010/Facial-Expression-Recognition.Pytorch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nfrom torch.autograd import Variable\n\nfrom PIL import Image\nfrom skimage import io\nfrom skimage.transform import resize as rs\n\ncut_size = 44\n\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%run \"./transforms\""],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\ncfg = {\n    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n}\n\n\nclass VGG(nn.Module):\n    def __init__(self, vgg_name):\n        super(VGG, self).__init__()\n        self.features = self._make_layers(cfg[vgg_name])\n        self.classifier = nn.Linear(512, 7)\n\n    def forward(self, x):\n        out = self.features(x)\n        out = out.view(out.size(0), -1)\n        out = F.dropout(out, p=0.5, training=self.training)\n        out = self.classifier(out)\n        return out\n\n    def _make_layers(self, cfg):\n        layers = []\n        in_channels = 3\n        for x in cfg:\n            if x == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n                           nn.BatchNorm2d(x),\n                           nn.ReLU(inplace=True)]\n                in_channels = x\n        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n        return nn.Sequential(*layers)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["model = '/dbfs/mnt/bs/Models/PrivateTest_model.t7'\nnet = VGG('VGG19')\ncheckpoint = torch.load(model, map_location='cpu')\n\nnet.load_state_dict(checkpoint['net'])\nnet.eval()  "],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["def rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n  \ntransform_test = Compose([\n    TenCrop(cut_size),\n    Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])),\n])\n  \ndef eval_pyTorch_emotion(image):\n  # prep image\n  gray = rgb2gray(image)\n  im_pil = Image.fromarray(gray)\n  gray = rs(gray, (48,48)).astype(np.uint8)\n  img = gray[:, :, np.newaxis]\n  img = np.concatenate((img, img, img), axis=2)\n  img = Image.fromarray(img)\n  inputs = transform_test(img)\n  class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n  ncrops, c, h, w = np.shape(inputs)\n  inputs = inputs.view(-1, c, h, w)\n  inputs = Variable(inputs, volatile=True)\n  outputs = net(inputs)\n  outputs_avg = outputs.view(ncrops, -1).mean(0)  # avg over crops\n  score = F.softmax(outputs_avg)\n  _, predicted = torch.max(outputs_avg.data, 0)\n  \n  #print (score)\n  #print (predicted)\n  #print(\"The Expression is %s\" %str(class_names[int(predicted.cpu().numpy())]))\n  return score\n  "],"metadata":{},"outputs":[],"execution_count":5}],"metadata":{"name":"PyTorch Emotion Eval","notebookId":270093129287856},"nbformat":4,"nbformat_minor":0}

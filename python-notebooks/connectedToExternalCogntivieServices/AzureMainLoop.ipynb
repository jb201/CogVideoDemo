{"cells":[{"cell_type":"markdown","source":["# Main Loop \n\nWill check an Azure Storage Location, look for new .mp4 video files and process those files through a pipeline"],"metadata":{}},{"cell_type":"code","source":["import cv2\nimport requests \nimport uuid\nimport os\nfrom pyspark.sql import *\nfrom datetime import datetime, timedelta\n\n# shouls be stored in databricks secrets backed by Azure Databrick secrets\n# \n# replace with your subscription key for face api \nsubscription_key = 'KEY_FOR_FACE_API'\nassert subscription_key\n\n\n# replace below with the correct location of face api account (need to test with containerized version)\nface_api_url = 'https://LOCATION.api.cognitive.microsoft.com/face/v1.0/detect?returnFaceLandmarks=true'\nheaders = { 'Ocp-Apim-Subscription-Key': subscription_key }\n\nsas_token ='REPLACE_WITH_STORAGE_SAS_KEY'\nstorage_source ='wasbs://BLOB_CONTAINER@STORAGE_ACCOUNT.blob.core.windows.net'\nstorage_sas_pointer = 'fs.azure.sas.BLOB_CONTAINER.STORAGE_ACCOUNT.blob.core.windows.net'\nspeech_transcript_key = \"REPLACE_WITH_SPEECH_API_KEY\"\nsub_key = \"REPLACE_WITH_SUBSCRIPTION_KEY\"\nspeech_key, service_region = speech_transcript_key, \"REPLACE_WITH_LOCATION\"\n\n# adjust to your parameter set     \nparams = {\n    'returnFaceId': 'true',\n    'returnFaceLandmarks': 'false',\n    'returnFaceAttributes': 'age,gender,headPose,smile,emotion,hair,makeup,occlusion,accessories,blur,exposure,noise',\n}\n\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%run \"./AudioProcessing\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%run \"./VideoProcessing\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# We should store this in a databrick secret \nmounts = dbutils.fs.mounts()\nmountPoint = next((x for x in mounts if x.mountPoint == '/mnt/bs/'), None)\nprint (mountPoint)\nif mountPoint == None:\n  dbutils.fs.mount(source =  storage_source,mount_point = '/mnt/bs/',extra_configs = {stoarge_sas_pointer: sas_token})\n  \n# If you need to unmount to refresh keys do htis\n# dbutils.fs.unmount(mount_point = '/mnt/bs/')"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["def ExtractWavFromMp4(inFile, wavFile):\n  command = \"ffmpeg -i {} -f wav -ac 1 -ar 16000 -vn -acodec pcm_s16le  {}\".format(inFile, wavFile)\n  output = os.system(command)\n  print(output)\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["def flatten_json(y):\n    out = {}\n\n    def flatten(x, name=''):\n        if type(x) is dict:\n            for a in x:\n                flatten(x[a], name + a + '_')\n        elif type(x) is list:\n            i = 0\n            for a in x:\n                flatten(a, name + str(i) + '_')\n                i += 1\n        else:\n            out[name[:-1]] = x\n\n    flatten(y)\n    return out"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["def persistAudioDataToTable(audio_output,videoId,tableName):\n  audio_output.write.mode(\"append\").saveAsTable(tableName)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["files = dbutils.fs.ls(\"/mnt/bs/IncomingVideos\")\n\nfor file in files:\n  path = '/' + file.path.replace(':','')\n  \n  if path.endswith('.mp4'):\n    print('Now Processing: ', path)\n    # add a record of the file being processed in a databricks table\n    # Extract Audio \n    newId = uuid.uuid4()\n    print (str(newId))\n    pathToAudio = '/dbfs/mnt/bs/IncomingVideos/{}.wav'.format(newId)\n    \n    print('Id of this run: ', newId)\n    print ('Extracting Audio to wav for Transcription')\n    ExtractWavFromMp4(path, pathToAudio)\n    print('Now extracting Face Info')\n    process_sample_frames(path,10, str(newId))\n    \n    print('Run Audio Processing - Speech To Text for {}'.format(pathToAudio))\n    output = speech_recognition_with_pull_stream(pathToAudio, str(newId)) \n    persistAudioDataToTable(output,newId,\"tempAudioTable\")\n    # Processing is complete --- copy blobs so they are not reprocessed \n    completePath = file.path.replace('IncomingVideos','IncomingVideos/completed')\n    dbutils.fs.cp(file.path, completePath)\n    dbutils.fs.rm(file.path)\n    realAudioPath = 'dbfs:/mnt/bs/IncomingVideos/{}.wav'.format(newId)\n    completeAudioPath = realAudioPath.replace('IncomingVideos','IncomingVideos/completed')\n    # move audio \n    print ('cleaning up')\n    dbutils.fs.cp(realAudioPath, completeAudioPath)\n    dbutils.fs.rm(realAudioPath)\n    sqlCommand = \"INSERT INTO table Videos VALUES('{}','{}','{}','{}','{}')\".format(newId, path, completePath,pathToAudio,completeAudioPath)\n    spark.sql(sqlCommand)"],"metadata":{},"outputs":[],"execution_count":9}],"metadata":{"name":"AzureMainLoop","notebookId":270093129287862},"nbformat":4,"nbformat_minor":0}
